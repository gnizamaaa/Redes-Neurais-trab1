{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a622311",
   "metadata": {},
   "source": [
    "\n",
    "# Trabalho 1: Diferenciação Automática com Grafos Computacionais\n",
    "\n",
    "## Informações Gerais\n",
    "\n",
    "- Data de Entrega: 28/11/2025\n",
    "- Pontuação: 10 pontos \n",
    "- O trabalho deve ser feito individualmente.\n",
    "- A entrega do trabalho deve ser realizada via sistema testr.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81844651",
   "metadata": {},
   "source": [
    "## Especificação\n",
    "\n",
    "⚠️ *Esta explicação assume que você leu e entendeu os slides sobre grafos computacionais.*\n",
    "\n",
    "O trabalho consiste em implementar um sistema de diferenciação automática usando grafos computacionais e utilizar este sistema para resolver um conjunto de problemas.\n",
    "\n",
    "Para isto, devem ser definidos um tipo Tensor para representar dados (similares aos arrays do numpy) e operações (e.g., soma, subtração, etc.) que geram tensores como saída. \n",
    "\n",
    "Sempre que uma operação é realizada, é armazenado no tensor de saída referências para os seus pais, isto é, os valores usados como entrada para a operação. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacc1a1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19261d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Any\n",
    "from collections.abc import Iterable\n",
    "from abc import ABC, abstractmethod\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284f531",
   "metadata": {},
   "source": [
    "### Classe NameManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd00b34",
   "metadata": {},
   "source": [
    "A classe NameManager provê uma forma conveniente de dar nomes intuitivos para tensores que resultam de operações. A idéia é tornar mais fácil para o usuário das demais classes qual operação gerou qual tensor. Ela provê os seguintes métodos públicos: \n",
    "\n",
    "- reset(): reinicia o sistema de gestão de nomes.\n",
    "- new(<basename>: str): retorna um nome único a partir do nome de base passado como argumento. \n",
    "  \n",
    "Como indicado no exemplo abaixo da classe, a idéia geral é que uma sequência de operações é feita, os nomes dos tensores sejam os nomes das operações seguidos de um número. Se forem feitas 3 operações de soma e uma de multiplicação, seus tensores de saída terão os nomes \"add:0\", \"add:1\", \"add:2\" e \"prod:0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162272a0",
   "metadata": {
    "tags": [
     "name_manager"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add:0\n",
      "in:0\n",
      "add:1\n",
      "add:2\n",
      "in:1\n",
      "prod:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NameManager:\n",
    "    _counts = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def reset():\n",
    "        NameManager._counts = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _count(name):\n",
    "        if name not in NameManager._counts:\n",
    "            NameManager._counts[name] = 0\n",
    "        count = NameManager._counts[name]\n",
    "        return count\n",
    "\n",
    "    @staticmethod\n",
    "    def _inc_count(name):\n",
    "        assert name in NameManager._counts, f'Name {name} is not registered.'\n",
    "        NameManager._counts[name] += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def new(name: str):\n",
    "        count = NameManager._count(name)\n",
    "        tensor_name = f\"{name}:{count}\"\n",
    "        NameManager._inc_count(name)\n",
    "        return tensor_name\n",
    "\n",
    "# exemplo de uso\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('in'))\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('in'))\n",
    "print(NameManager.new('prod'))\n",
    "\n",
    "NameManager.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69485a9",
   "metadata": {},
   "source": [
    "### Classe Tensor\n",
    "\n",
    "Deve ser criada uma classe `Tensor` representando um array multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "448496d7",
   "metadata": {
    "tags": [
     "tensor"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tensor:\n",
    "    \"\"\"Classe representando um array multidimensional.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "    - _arr  (privado): dados internos do tensor como\n",
    "        um array do numpy com 2 dimensões (ver Regras)\n",
    "\n",
    "    - _parents (privado): lista de tensores que foram\n",
    "        usados como argumento para a operação que gerou o\n",
    "        tensor. Será vazia se o tensor foi inicializado com\n",
    "        valores diretamente. Por exemplo, se o tensor foi\n",
    "        resultado da operação a + b entre os tensores a e b,\n",
    "        _parents = [a, b].\n",
    "\n",
    "    - requires_grad (público): indica se devem ser\n",
    "        calculados gradientes para o tensor ou não.\n",
    "\n",
    "    - grad (público): Tensor representando o gradiente.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 # Dados do tensor. Além dos tipos listados,\n",
    "                 # arr também pode ser do tipo Tensor.\n",
    "                 arr: Union[np.ndarray, list, numbers.Number, Any],\n",
    "                 # Entradas da operacao que gerou o tensor.\n",
    "                 # Deve ser uma lista de itens do tipo Tensor.\n",
    "                 parents: list[Any] = [],\n",
    "                 # se o tensor requer o calculo de gradientes ou nao\n",
    "                 requires_grad: bool = True,\n",
    "                 # nome do tensor\n",
    "                 name: str = '',\n",
    "                 # referência para um objeto do tipo Operation (ou\n",
    "                 # subclasse) indicando qual operação gerou este\n",
    "                 # tensor. Este objeto também possui um método\n",
    "                 # para calcular a derivada da operação.\n",
    "                 operation=None):\n",
    "        # Guardando o tensor como uma matriz do numpy\n",
    "        # self._arr = np.matrix([arr], dtype=float)\n",
    "        if isinstance(arr, Tensor):\n",
    "            self._arr = arr._arr.copy()\n",
    "        else:\n",
    "            np_arr = np.array(arr, dtype=float)\n",
    "            if np_arr.ndim == 0:\n",
    "                self._arr = np.matrix(np_arr).reshape((1,1))\n",
    "            elif np_arr.ndim == 1:\n",
    "                self._arr = np.matrix(np_arr).reshape((np_arr.shape[0], 1))\n",
    "            elif np_arr.ndim == 2:\n",
    "                self._arr = np.matrix(np_arr)\n",
    "            else:\n",
    "                raise ValueError(\"O array deve ter no máximo 2 dimensões.\")\n",
    "\n",
    "\n",
    "\n",
    "        self.parents = parents if parents is not None else []\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = np.matrix(np.zeros(self._arr.shape))\n",
    "        self._operation = operation\n",
    "        self._name = name if name else NameManager.new('in')\n",
    "        \"\"\"Construtor\n",
    "\n",
    "        O construtor deve permitir a criacao de tensores das seguintes formas:\n",
    "\n",
    "            # a partir de escalares\n",
    "            x = Tensor(3)\n",
    "\n",
    "            # a partir de listas\n",
    "            x = Tensor([1,2,3])\n",
    "\n",
    "            # a partir de arrays\n",
    "            x = Tensor(np.array([1,2,3]))\n",
    "\n",
    "            # a partir de outros tensores (construtor de copia)\n",
    "            x = Tensor(Tensor(np.array([1,2,3])))\n",
    "\n",
    "        Para isto, as seguintes regras devem ser obedecidas:\n",
    "\n",
    "        - Se o argumento arr não for um array do numpy,\n",
    "            ele deve ser convertido em um. Defina o dtype do\n",
    "            array como float de forma a permitir que NÃO seja\n",
    "            necessário passar constantes float como Tensor(3.0),\n",
    "            mas possamos criar um tensor apenas com Tensor(3).\n",
    "\n",
    "        - O atributo _arr deve ser uma matriz, isto é,\n",
    "            ter 2 dimensões (ver Regras).\n",
    "\n",
    "        - Se o argumento arr for um Tensor, ele deve ser\n",
    "            copiado (cuidado com cópias por referência).\n",
    "\n",
    "        - Se arr for um array do numpy com 1 dimensão,\n",
    "            ele deve ser convertido em uma matriz coluna.\n",
    "\n",
    "        - Se arr for um array do numpy com dimensão maior\n",
    "            que 2, deve ser lançada uma exceção.\n",
    "\n",
    "        - Tensores que não foram produzidos como resultado\n",
    "            de uma operação não têm pais nem operação.\n",
    "            Os nomes destes tensores devem seguir o formato in:3.\n",
    "        \"\"\"\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Reinicia o gradiente com zero\"\"\"\n",
    "        self.grad = np.matrix(np.zeros(self._arr.shape))\n",
    "\n",
    "    def numpy(self):\n",
    "        \"\"\"Retorna o array interno\"\"\"\n",
    "        return self._arr.copy()\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Permite visualizar os dados do tensor como string\"\"\"\n",
    "        return f\"Tensor({self._arr}, name={self._name}, shape={self._arr.shape})\"\n",
    "\n",
    "    def backward(self, my_grad=None):\n",
    "        \"\"\"Método usado tanto iniciar o processo de\n",
    "        diferenciação automática, quanto por um filho\n",
    "        para enviar o gradiente do pai. No primeiro\n",
    "        caso, o argumento my_grad não será passado.\n",
    "        \"\"\"\n",
    "\n",
    "        if my_grad is None:\n",
    "            # Iniciando o processo de backpropagation\n",
    "            assert self._operation is not None, \"Não é possível iniciar backpropagation em um tensor que não foi gerado por uma operação.\"\n",
    "            # Gradiente inicial é uma matriz identidade ou é sempre uma matriz 1?\n",
    "            init_grad = np.matrix(np.ones(self._arr.shape))\n",
    "\n",
    "            grads = self._operation.grad(\n",
    "                Tensor(init_grad, name=NameManager.new('init_grad')),\n",
    "                *self.parents)\n",
    "            \n",
    "            for parent, grad in zip(self.parents, grads):\n",
    "                if parent.requires_grad:\n",
    "                    parent.backward(grad)\n",
    "        else:\n",
    "            # Recebendo o gradiente de um filho\n",
    "            ## Print tudo para debug\n",
    "            print(f\"Backpropagating on Tensor {self._name}\")\n",
    "            print(f\"My grad shape: {my_grad.numpy().shape}\")\n",
    "            print(f\"My grad:\\n{my_grad.numpy()}\")\n",
    "            print(f\"Current grad shape: {self.grad.shape}\")\n",
    "            print(f\"Current grad:\\n{self.grad}\")\n",
    "\n",
    "            self.grad += my_grad.numpy() # acumulando pq pode ter mais de um filho (obs p eu n tirar dnv)\n",
    "            if self._operation is not None:\n",
    "                grads = self._operation.grad(\n",
    "                    my_grad,\n",
    "                    *self.parents)\n",
    "                for parent, grad in zip(self.parents, grads):\n",
    "                    if parent.requires_grad:\n",
    "                        parent.backward(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c612fc",
   "metadata": {},
   "source": [
    "### Interface de  Operações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db44044",
   "metadata": {},
   "source": [
    "A classe abaixo define a interface que as operações devem implementar. Ela não precisa ser modificada, mas pode, caso queira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a19b73",
   "metadata": {
    "tags": [
     "op"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Op(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando as entradas e\n",
    "            retorna o tensor resultado. O método deve\n",
    "            garantir que o atributo parents do tensor\n",
    "            de saída seja uma lista de tensores.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna os gradientes dos pais em como tensores.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "        - back_grad: Derivada parcial em relação à saída\n",
    "            da operação backpropagada pelo filho.\n",
    "\n",
    "        - args: variaveis de entrada da operacao (pais)\n",
    "            como tensores.\n",
    "\n",
    "        - O nome dos tensores de gradiente devem ter o\n",
    "            nome da operacao seguido de '_grad'.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89e386",
   "metadata": {},
   "source": [
    "### Implementação das Operações\n",
    "\n",
    "Operações devem herdar de `Op` e implementar os métodos `__call__` e `grad`.\n",
    "\n",
    "Pelo menos as seguintes operações devem ser implementadas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa4f7719",
   "metadata": {
    "tags": [
     "add"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Add(Op):\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        # args = self._ts(args)\n",
    "        args = [arg if isinstance(arg, Tensor) else Tensor(arg) for arg in args]\n",
    "        result = args[0].numpy() + args[1].numpy()\n",
    "        return Tensor(result, parents=args, name=NameManager.new('add'),\n",
    "                      operation=self)\n",
    "\n",
    "    def grad(self, back_grad, *args, **kwargs) -> list[Tensor]:\n",
    "        return [Tensor(back_grad, name=NameManager.new('add_grad')),\n",
    "                Tensor(back_grad, name=NameManager.new('add_grad'))] \n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "add = Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05cb44e6",
   "metadata": {
    "tags": [
     "sub"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sub(Op):\n",
    "    \"\"\"Sub(a, b): a - b\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        # args = self._ts(args)\n",
    "        result = args[0].numpy() - args[1].numpy()\n",
    "        return Tensor(result, parents=args, name=NameManager.new('sub'),\n",
    "                      operation=self)\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        return [Tensor(back_grad, name=NameManager.new('sub_grad')),\n",
    "                Tensor(-back_grad, name=NameManager.new('sub_grad'))]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sub = Sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53df08",
   "metadata": {
    "tags": [
     "prod"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Prod(Op):\n",
    "    \"\"\"Prod(a, b): produto ponto a ponto de a e b ou produto escalar-tensor\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "prod = Prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3838a7",
   "metadata": {
    "tags": [
     "sin"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sin(Op):\n",
    "    \"\"\"seno element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        args = self._ts(args)\n",
    "        # arr = args[0].numpy()\n",
    "        # for i in arr:\n",
    "        #     i = np.sin(i)\n",
    "        \n",
    "        # Havia esquecido que numpy consegue fzr direto do array\n",
    "        arr = np.sin(args[0].numpy())\n",
    "\n",
    "        return Tensor(arr, parents=args, name=NameManager.new('sin'),\n",
    "                      operation=self)\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        return [Tensor(back_grad, name=NameManager.new('sin_grad')) * Tensor(np.cos(args[0].numpy()), name=NameManager.new('cos'))]\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sin = Sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cb8ef",
   "metadata": {
    "tags": [
     "cos"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Cos(Op):\n",
    "    \"\"\"cosseno element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "cos = Cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eac52c",
   "metadata": {
    "tags": [
     "sum"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sum(Op):\n",
    "    \"\"\"Retorna a soma dos elementos do tensor\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "# ⚠️ vamos chamar de my_sum porque python ja possui uma funcao sum\n",
    "my_sum = Sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098a39c",
   "metadata": {
    "tags": [
     "mean"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Mean(Op):\n",
    "    \"\"\"Retorna a média dos elementos do tensor\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "mean = Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37692879",
   "metadata": {
    "tags": [
     "square"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Square(Op):\n",
    "    \"\"\"Eleva cada elemento ao quadrado\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "square = Square()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6542807d",
   "metadata": {
    "tags": [
     "matmul"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class MatMul(Op):\n",
    "    \"\"\"MatMul(A, B): multiplicação de matrizes\n",
    "\n",
    "    C = A @ B\n",
    "    de/dA = de/dc @ B^T\n",
    "    de/dB = A^T @ de/dc\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "matmul = MatMul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a38e3",
   "metadata": {
    "tags": [
     "exp"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Exp(Op):\n",
    "    \"\"\"Exponenciação element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "exp = Exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc813e",
   "metadata": {
    "tags": [
     "relu"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReLU(Op):\n",
    "    \"\"\"ReLU element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "relu = ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae499275",
   "metadata": {
    "tags": [
     "sigmoid"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sigmoid(Op):\n",
    "    \"\"\"Sigmoid element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sigmoid = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce464ba",
   "metadata": {
    "tags": [
     "tanh"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tanh(Op):\n",
    "    \"\"\"Tanh element-wise\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "tanh = Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a37eb",
   "metadata": {
    "tags": [
     "softmax"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Softmax(Op):\n",
    "    \"\"\"Softmax de um array de valores. Lembre-se que cada elemento do array influencia o resultado da função para todos os demais elementos.\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        \"\"\"Realiza a operação usando os argumentos dados em args\"\"\"\n",
    "\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> list[Tensor]:\n",
    "        \"\"\"Retorna a lista de derivadas parciais em relação aos pais (passados em args)\"\"\"\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "softmax = Softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12256fd7",
   "metadata": {},
   "source": [
    "\n",
    "### ‼️ Regras e Pontos de Atenção‼️\n",
    "\n",
    "- Vamos fazer a hipótese simplificadora que Tensores devem ser sempre matrizes. Por exemplo, o escalar 2 deve ser armazado em `_arr` como a matriz `[[2]]`. De forma similar, a lista `[1, 2, 3]` deve ser armazenada em `_arr` como em uma matriz coluna.\n",
    "\n",
    "- Devem ser realizados `asserts` nas operações para garantir que os shapes dos operandos fazem sentido. Esta verificação também deve ser feita depois das operações que manipulam gradientes de tensores.\n",
    "\n",
    "- Devem ser respeitados os nomes dos atributos, métodos e classes para viabilizar os testes automáticos.\n",
    "\n",
    "- Gradientes devem ser calculados usando uma passada pelo grafo computacional.\n",
    "\n",
    "- Os gradientes devem ser somados e não substituídos nas chamadas de  backward. Isto vai permitir que os gradientes sejam acumulados entre amostras do dataset e que os resultados sejam corretos mesmo em caso de ramificações e junções no grafo computacional.\n",
    "\n",
    "- Lembre-se de zerar os gradientes após cada passo de gradient descent (atualização dos parâmetros).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc927248",
   "metadata": {},
   "source": [
    "## Testes Básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae08a8",
   "metadata": {},
   "source": [
    "Estes testes avaliam se a derivada da função está sendo calculada corretamente, mas em muitos casos **não** avaliam se os gradientes backpropagados estão sendo incorporados corretamente. Esta avaliação será feita nos problemas da próxima seção."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05318a9",
   "metadata": {},
   "source": [
    "Operador de Soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd20550",
   "metadata": {
    "tags": [
     "test_add"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backpropagating on Tensor add:10\n",
      "My grad shape: (3, 1)\n",
      "My grad:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Current grad shape: (3, 1)\n",
      "Current grad:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Backpropagating on Tensor in:19\n",
      "My grad shape: (3, 1)\n",
      "My grad:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Current grad shape: (3, 1)\n",
      "Current grad:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Backpropagating on Tensor in:20\n",
      "My grad shape: (3, 1)\n",
      "My grad:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Current grad shape: (3, 1)\n",
      "Current grad:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Backpropagating on Tensor in:21\n",
      "My grad shape: (3, 1)\n",
      "My grad:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Current grad shape: (1, 1)\n",
      "Current grad:\n",
      "[[0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (3,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m c = add(a, b)\n\u001b[32m      6\u001b[39m d = add(c, \u001b[32m3.0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# c.backward()\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# esperado: matrizes coluna contendo 1\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(a.grad)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, my_grad)\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m parent, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.parents, grads):\n\u001b[32m    132\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m parent.requires_grad:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m             \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    135\u001b[39m     \u001b[38;5;66;03m# Recebendo o gradiente de um filho\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m## Print tudo para debug\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBackpropagating on Tensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, my_grad)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent grad shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.grad.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    141\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent grad:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.grad\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_grad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# acumulando pq pode ter mais de um filho (obs p eu n tirar dnv)\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._operation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    145\u001b[39m     grads = \u001b[38;5;28mself\u001b[39m._operation.grad(\n\u001b[32m    146\u001b[39m         my_grad,\n\u001b[32m    147\u001b[39m         *\u001b[38;5;28mself\u001b[39m.parents)\n",
      "\u001b[31mValueError\u001b[39m: non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (3,1)"
     ]
    }
   ],
   "source": [
    "# add\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = add(a, b)\n",
    "d = add(c, 3.0) # Soma de escalar em vetor - #TODO \n",
    "d.backward()\n",
    "# c.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=in_grad, shape=(3, 1))\n",
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# add\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = add(a, b)\n",
    "d = add(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac72b1a",
   "metadata": {},
   "source": [
    "Operador de Subtração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612377aa",
   "metadata": {
    "tags": [
     "test_sub"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[1.]\n",
      " [1.]\n",
      " [1.]], name=in_grad, shape=(3, 1))\n",
      "Tensor([[-1.]\n",
      " [-1.]\n",
      " [-1.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# sub\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = sub(a, b)\n",
    "d = sub(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1 e -1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c8e63",
   "metadata": {},
   "source": [
    "Operador de Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60de82",
   "metadata": {
    "tags": [
     "test_prod"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[12.]\n",
      " [15.]\n",
      " [18.]], name=in_grad, shape=(3, 1))\n",
      "Tensor([[3.]\n",
      " [6.]\n",
      " [9.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# prod\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = prod(a, b)\n",
    "d = prod(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: [12, 15, 18]^T\n",
    "print(a.grad)\n",
    "# esperado: [3, 6, 9]^T\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91e1c3",
   "metadata": {},
   "source": [
    "Operadores trigonométricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185a989",
   "metadata": {
    "tags": [
     "test_sin_cos"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[-1.]\n",
      " [ 1.]\n",
      " [-1.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# sin e cos\n",
    "\n",
    "a = Tensor([np.pi, 0, np.pi/2])\n",
    "b = sin(a)\n",
    "c = cos(a)\n",
    "d = my_sum(add(b, c))\n",
    "d.backward()\n",
    "\n",
    "# esperado: [-1, 1, -1]^T\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29f232",
   "metadata": {
    "tags": [
     "test_sum"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Sum\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = add(prod(a, 3.0), a)\n",
    "c = my_sum(b)\n",
    "c.backward()\n",
    "\n",
    "# esperado: [4, 4, 4, 4]^T\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943e71a",
   "metadata": {
    "tags": [
     "test_mean"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.25]\n",
      " [0.25]\n",
      " [0.25]\n",
      " [0.25]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Mean\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = mean(a)\n",
    "b.backward()\n",
    "\n",
    "# esperado: [0.25, 0.25, 0.25, 0.25]^T\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7dbd2c",
   "metadata": {
    "tags": [
     "test_square"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[9.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]], name=square:0, shape=(4, 1))\n",
      "Tensor([[6.]\n",
      " [2.]\n",
      " [0.]\n",
      " [4.]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Square\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = square(a)\n",
    "\n",
    "# esperado: [9, 1, 0, 4]^T\n",
    "print(b)\n",
    "\n",
    "b.backward()\n",
    "\n",
    "# esperado: [6, 2, 0, 4]\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2ead7",
   "metadata": {
    "tags": [
     "test_matmul"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[14.]\n",
      " [32.]\n",
      " [50.]], name=matmul:0, shape=(3, 1))\n",
      "Tensor([[1. 2. 3.]\n",
      " [1. 2. 3.]\n",
      " [1. 2. 3.]], name=in_grad, shape=(3, 3))\n",
      "Tensor([[12.]\n",
      " [15.]\n",
      " [18.]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# matmul\n",
    "\n",
    "W = Tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]\n",
    "])\n",
    "\n",
    "v = Tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "z = matmul(W, v)\n",
    "\n",
    "# esperado: [14, 32, 50]^T\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "# esperado:\n",
    "# [1, 2, 3]\n",
    "# [1, 2, 3]\n",
    "# [1, 2, 3]\n",
    "print(W.grad)\n",
    "\n",
    "# esperado: [12, 15, 18]^T\n",
    "print(v.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706212d2",
   "metadata": {
    "tags": [
     "test_exp"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[ 2.71828183]\n",
      " [ 7.3890561 ]\n",
      " [20.08553692]], name=exp:0, shape=(3, 1))\n",
      "Tensor([[ 2.71828183]\n",
      " [ 7.3890561 ]\n",
      " [20.08553692]], name=in_grad, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# Exp\n",
    "\n",
    "v = Tensor([1.0, 2.0, 3.0])\n",
    "w = exp(v)\n",
    "\n",
    "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9510d010",
   "metadata": {
    "tags": [
     "test_relu"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]], name=relu:0, shape=(4, 1))\n",
      "Tensor([[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Relu\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = relu(v)\n",
    "\n",
    "# esperado: [0, 0, 1, 3]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0, 0, 1, 1]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f0fbf8d",
   "metadata": {
    "tags": [
     "test_sigmoid"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.26894142]\n",
      " [0.5       ]\n",
      " [0.73105858]\n",
      " [0.95257413]], name=sigmoid:0, shape=(4, 1))\n",
      "Tensor([[0.19661193]\n",
      " [0.25      ]\n",
      " [0.19661193]\n",
      " [0.04517666]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = sigmoid(v)\n",
    "\n",
    "# esperado: [0.268.., 0.5, 0.731.., 0.952..]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0.196..., 0.25, 0.196..., 0.045...]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e867dec",
   "metadata": {
    "tags": [
     "test_tanh"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[-0.76159416]\n",
      " [ 0.        ]\n",
      " [ 0.76159416]\n",
      " [ 0.99505475]], name=tanh:0, shape=(4, 1))\n",
      "Tensor([[0.41997434]\n",
      " [1.        ]\n",
      " [0.41997434]\n",
      " [0.00986604]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Tanh\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = tanh(v)\n",
    "\n",
    "# esperado: [[-0.76159416, 0., 0.76159416, 0.99505475]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0.41997434, 1., 0.41997434, 0.00986604]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3235d",
   "metadata": {
    "tags": [
     "test_softmax"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.00381737]\n",
      " [0.13970902]\n",
      " [0.23034123]\n",
      " [0.62613238]], name=softmax:6, shape=(4, 1))\n",
      "MSE: Tensor([[0.36424932]], name=mean:7, shape=(1, 1))\n",
      "Tensor([[-0.00278095]\n",
      " [-0.02243068]\n",
      " [-0.02654377]\n",
      " [ 0.05175539]], name=in_grad, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "\n",
    "x = Tensor([-3.1, 0.5, 1.0, 2.0])\n",
    "y = softmax(x)\n",
    "\n",
    "# esperado: [0.00381737, 0.13970902, 0.23034123, 0.62613238]^T\n",
    "print(y)\n",
    "\n",
    "# como exemplo, calcula o MSE para um target vector\n",
    "diff = sub(y, [1, 0, 0, 0])\n",
    "sq = square(diff)\n",
    "a = mean(sq)\n",
    "\n",
    "# esperado: 0.36424932\n",
    "print(\"MSE:\", a)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "# esperado: [-0.00278095, -0.02243068, -0.02654377, 0.05175539]^T\n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a3d9f",
   "metadata": {},
   "source": [
    "\n",
    "## Referências\n",
    "\n",
    "### Principais\n",
    "\n",
    "- [Build your own pytorch](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-1-Computation-Graphs/)\n",
    "- [Build your own Pytorch - 2: Backpropagation](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-2-Autograd/)\n",
    "- [Build your own PyTorch - 3: Training a Neural Network with self-made AD software](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-3-Build-Classifier/)\n",
    "- [Pytorch: A Gentle Introduction to torch.autograd](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "- [Automatic Differentiation with torch.autograd](https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "\n",
    "### Secundárias\n",
    "\n",
    "- [Tom Roth: Building a computational graph: part 1](https://tomroth.dev/compgraph1/)\n",
    "- [Tom Roth: Building a computational graph: part 2](https://tomroth.dev/compgraph2/)\n",
    "- [Tom Roth: Building a computational graph: part 3](https://tomroth.dev/compgraph3/)\n",
    "- [Roger Grosse (Toronto) class on Automatic Differentiation](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf)\n",
    "- [Computational graphs and gradient flows](https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html)\n",
    "- [Colah Visual Blog: Backprop](https://colah.github.io/posts/2015-08-Backprop/)\n",
    "- [Towards Data Science: Automatic Differentiation (AutoDiff): A Brief Intro with Examples](https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b/)\n",
    "- [A Hands-on Introduction to Automatic Differentiation - Part 1](https://mostafa-samir.github.io/auto-diff-pt1/)\n",
    "- [Build Your own Deep Learning Framework - A Hands-on Introduction to Automatic Differentiation - Part 2](https://mostafa-samir.github.io/auto-diff-pt1/)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
